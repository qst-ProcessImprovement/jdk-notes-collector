<!-- 
RSS generated by JIRA (9.12.27#9120027-sha1:edc4490121e366e9e7bd2213d532dbe7e028fc5d) at Tue Sep 30 08:51:44 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>Java Bug System</title>
    <link>https://bugs.openjdk.org</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-us</language>    <build-info>
        <version>9.12.27</version>
        <build-number>9120027</build-number>
        <build-date>02-09-2025</build-date>
    </build-info>


<item>
            <title>[JDK-8303215] Make thread stacks not use huge pages</title>
                <link>https://bugs.openjdk.org/browse/JDK-8303215</link>
                <project id="10100" key="JDK">JDK</project>
                    <description>When a system has Transparent Huge Pages (THP) enabled (/sys/kernel/mm/transparent_hugepage/enabled is set to &amp;#39;always&amp;#39;), thread stacks can have  significantly more resident set size (RSS) than they actually require. This occurs when the stack size is 2MB or larger, which makes the memory range of the stack more likely to be aligned on a 2MB boundary. This in turn makes the stack eligible to be backed by huge pages resulting in more memory consumption than it would otherwise when standard small pages are used. This issue is more apparent on AArch64 platforms where the default stack size is 2MB.&lt;br/&gt;
&lt;br/&gt;
The JVM should explicitly make the thread stacks not use huge pages. This can be done by calling &amp;#39;madvise&amp;#39; on the stack memory with MADV_NOHUGEPAGE advice instructing the kernel not to back the stack address range with huge pages.&lt;br/&gt;
&lt;br/&gt;
</description>
                <environment></environment>
        <key id="5095026">JDK-8303215</key>
            <summary>Make thread stacks not use huge pages</summary>
                <type id="7" iconUrl="https://bugs.openjdk.org/secure/viewavatar?size=xsmall&amp;avatarId=14707&amp;avatarType=issuetype">Enhancement</type>
                                            <priority id="3" iconUrl="https://bugs.openjdk.org/images/jbsImages/p3.png">P3</priority>
                        <status id="5" iconUrl="https://bugs.openjdk.org/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="success"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="poonam">Poonam Bajaj Parhar</assignee>
                                    <reporter username="poonam">Poonam Bajaj Parhar</reporter>
                        <labels>
                            <label>8bpr-critical-approved</label>
                            <label>jdk17u-fix-request</label>
                            <label>jdk17u-fix-yes</label>
                            <label>large-pages</label>
                            <label>redhat-interest</label>
                            <label>threads</label>
                    </labels>
                <created>Sun, 26 Feb 2023 08:18:46 -0800</created>
                <updated>Wed, 25 Jun 2025 06:26:11 -0700</updated>
                            <resolved>Fri, 2 Jun 2023 06:32:58 -0700</resolved>
                                    <version>21</version>
                                    <fixVersion>21</fixVersion>
                                    <component>hotspot</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>16</watches>
                                                                                                                <comments>
                            <comment id="14620491" author="roboduke" created="Tue, 24 Oct 2023 00:15:07 -0700"  >A pull request was submitted for review.&lt;br/&gt;
Branch: master&lt;br/&gt;
URL: &lt;a href=&quot;https://git.openjdk.org/jdk11u-dev/pull/2208&quot;&gt;https://git.openjdk.org/jdk11u-dev/pull/2208&lt;/a&gt;&lt;br/&gt;
Date: 2023-10-24 06:22:34 +0000</comment>
                            <comment id="14606039" author="roboduke" created="Wed, 23 Aug 2023 01:27:45 -0700"  >A pull request was submitted for review.&lt;br/&gt;
URL: &lt;a href=&quot;https://git.openjdk.org/jdk11u-dev/pull/2086&quot;&gt;https://git.openjdk.org/jdk11u-dev/pull/2086&lt;/a&gt;&lt;br/&gt;
Date: 2023-08-17 14:50:49 +0000</comment>
                            <comment id="14597287" author="stuefe" created="Wed, 19 Jul 2023 08:55:07 -0700"  >Thanks for your initial investigation, your insights saved me a lot of time.</comment>
                            <comment id="14597285" author="poonam" created="Wed, 19 Jul 2023 08:41:28 -0700"  >We had thought about this problem but had considered it to be a rare scenario. Thanks for addressing this issue.</comment>
                            <comment id="14596913" author="stuefe" created="Tue, 18 Jul 2023 08:35:23 -0700"  >Unfortunately, this fix is not sufficient to prevent huge page coalescation of thread stacks that are adjacent and hence have their VMAs being merged by the kernel; see &lt;a href=&quot;https://bugs.openjdk.org/browse/JDK-8312182&quot; title=&quot;THPs cause huge RSS due to thread start timing issue&quot; class=&quot;issue-link&quot; data-issue-key=&quot;JDK-8312182&quot;&gt;&lt;strike&gt;JDK-8312182&lt;/strike&gt;&lt;/a&gt;</comment>
                            <comment id="14591598" author="mdoerr" created="Fri, 23 Jun 2023 01:36:56 -0700"  >Thanks for the hint! I guess we should backport that one later, too.</comment>
                            <comment id="14591544" author="stuefe" created="Thu, 22 Jun 2023 21:33:14 -0700"  >Note that this fix is incomplete; see &lt;a href=&quot;https://bugs.openjdk.org/browse/JDK-8310687&quot; title=&quot;JDK-8303215 is incomplete&quot; class=&quot;issue-link&quot; data-issue-key=&quot;JDK-8310687&quot;&gt;&lt;strike&gt;JDK-8310687&lt;/strike&gt;&lt;/a&gt;. It makes definitely sense to downport it in this state though, since it is already useful in the majority of cases.</comment>
                            <comment id="14591447" author="mdoerr" created="Thu, 22 Jun 2023 09:15:23 -0700"  >Fix Request (17u): &lt;br/&gt;
Should get backported for parity with 17.0.9-oracle. Applies cleanly.</comment>
                            <comment id="14591435" author="roboduke" created="Thu, 22 Jun 2023 08:46:40 -0700"  >A pull request was submitted for review.&lt;br/&gt;
URL: &lt;a href=&quot;https://git.openjdk.org/jdk17u-dev/pull/1492&quot;&gt;https://git.openjdk.org/jdk17u-dev/pull/1492&lt;/a&gt;&lt;br/&gt;
Date: 2023-06-22 15:37:52 +0000</comment>
                            <comment id="14586624" author="dukebot" created="Fri, 2 Jun 2023 06:32:57 -0700"  >Changeset: 59d9d9fc&lt;br/&gt;
Author:    Poonam Bajaj &amp;lt;&lt;a href=&apos;mailto:poonam@openjdk.org&apos;&gt;poonam@openjdk.org&lt;/a&gt;&amp;gt;&lt;br/&gt;
Date:      2023-06-02 13:32:22 +0000&lt;br/&gt;
URL:       &lt;a href=&quot;https://git.openjdk.org/jdk/commit/59d9d9fcb93c26dd8931d70934b889245b050acc&quot;&gt;https://git.openjdk.org/jdk/commit/59d9d9fcb93c26dd8931d70934b889245b050acc&lt;/a&gt;&lt;br/&gt;
</comment>
                            <comment id="14583867" author="roboduke" created="Tue, 23 May 2023 11:09:34 -0700"  >A pull request was submitted for review.&lt;br/&gt;
URL: &lt;a href=&quot;https://git.openjdk.org/jdk/pull/14105&quot;&gt;https://git.openjdk.org/jdk/pull/14105&lt;/a&gt;&lt;br/&gt;
Date: 2023-05-23 18:01:51 +0000</comment>
                            <comment id="14583851" author="poonam" created="Tue, 23 May 2023 11:06:48 -0700"  >[~stuefe] After having several rounds of internal discussions, we came to a conclusion to address this issue by adjusting the thread stack size on aarch64 systems in order to reduce the chances of stacks getting large-page aligned. Please see more details in this pull request: &lt;a href=&quot;https://github.com/openjdk/jdk/pull/14105&quot;&gt;https://github.com/openjdk/jdk/pull/14105&lt;/a&gt;&lt;br/&gt;
</comment>
                            <comment id="14574393" author="stuefe" created="Mon, 17 Apr 2023 21:33:02 -0700"  >[~poonam] I spoke with Florian Weimer yesterday and going forward he plans on adding a new thread attribute that would advice the glibc to not use THPs when mapping the stacks for new threads. That sounds like a good solution. Someone at Oracle ran into a similar problem and they tried to get a patch into glibc creating a new tunable: &lt;a href=&quot;https://patchwork.ozlabs.org/project/glibc/patch/20230320154155.549206-2-cupertino.miranda@oracle.com/&quot;&gt;https://patchwork.ozlabs.org/project/glibc/patch/20230320154155.549206-2-cupertino.miranda@oracle.com/&lt;/a&gt; but after my talk with Florian he leans toward adding an attribute instead since that will be easier to control for applications like the Java VM.&lt;br/&gt;
&lt;br/&gt;
Still does not help us with older glibc versions, but I believe your patch may work. One thing to think about is to make the stack for madvise large enough to also handle the occasional signal, but your 2MB should be fine. I also did ask Florian whether pthread TLS slots - that live on the original stack - is accessed via the stack pointer, and this seems to have been the case with the old linuxthreads implementation but not with nptl anymore.&lt;br/&gt;
&lt;br/&gt;
Security mechanism: Sorry for the vagueness, its probably nothing. I wondered whether hardened Linux variants observe irregularities like entering the kernel with a modified stack pointer. But probably not since the kernel does not know about pthread stacks.&lt;br/&gt;
</comment>
                            <comment id="14574337" author="poonam" created="Mon, 17 Apr 2023 10:51:05 -0700"  >[~stuefe] Thanks for taking a look at the patch. Yes, this is just a prototype and not the final changeset. As for mmap:ing and munmap:ing a temp stack before and after the madvise call, yes, we can do that but I am afraid that it can cause memory fragmentation.&lt;br/&gt;
&lt;br/&gt;
Could you please elaborate on your security mechanism point?</comment>
                            <comment id="14572607" author="stuefe" created="Sun, 9 Apr 2023 00:13:56 -0700"  >[~poonam] Interesting solution. I like that its static assembly, so no runtime dependency on stub generation. I thought also about switching stacks, then was looking into misusing sigaltstack(), but that is a global setting that interferes with normal signal handling. Your way is better.&lt;br/&gt;
&lt;br/&gt;
The main thing I&amp;#39;d be worried about would be glibc or kernel making implicit assumptions about us running on the original thread stack. Things like glibc accessing its TLS variables, which live at the start of the stack. But madvise is just a system call, so I think we don&amp;#39;t do much in glibc.&lt;br/&gt;
&lt;br/&gt;
We may also trigger security mechanisms here; this feels like something SE_LINUX or similar product may flag as dangerous.&lt;br/&gt;
&lt;br/&gt;
The synchronization needed is of course regrettable, this is just a prototype, right? No reason to not just locally mmap and unmap a temp stack again before/after the madvise call.</comment>
                            <comment id="14572464" author="poonam" created="Fri, 7 Apr 2023 09:10:09 -0700"  >[~stuefe] I have a prototype for the approach of calling madvise() on an alternate temporary stack: &lt;a href=&quot;https://github.com/openjdk/jdk/compare/master...poonamparhar:jdk:JDK-8303215?expand=1&quot;&gt;https://github.com/openjdk/jdk/compare/master...poonamparhar:jdk:JDK-8303215?expand=1&lt;/a&gt;. Would like to hear your thoughts on it.</comment>
                            <comment id="14572150" author="stuefe" created="Wed, 5 Apr 2023 22:04:19 -0700"  >[~dholmes] Thanks for clarifying. I assumed Thread::join was implemented via pthread_join. I see now that notification works via the Thread lock. So pthread_join would be available for something like stack management. But thinking this through, maybe not: we&amp;#39;d arrive at the same solution we have with fork-join, where we&amp;#39;d need a separate thread per thread for monitoring thread death.</comment>
                            <comment id="14572116" author="dholmes" created="Wed, 5 Apr 2023 17:00:26 -0700"  >[~stuefe] the VM does not use pthread_join. The only use of pthread_join in the JDK is in the launcher, where the main process thread joins the thread it created to load the VM.</comment>
                            <comment id="14572005" author="stuefe" created="Wed, 5 Apr 2023 07:16:10 -0700"  >[~poonam] Thank you for the excellent explanation. I also brushed up my THP knowledge and did some tests, and now I understand the problem better.&lt;br/&gt;
&lt;br/&gt;
When doing tests in THP=always mode I was struck by how large the variance in RSS was for the same run. I stabilized the rest of the VMs RSS usage (AlwaysPreTouch), and starting 1000 threads with a 2MB stack size gave me, on Linux x64, RSS numbers between 500MB and 900MB. With THP disabled, it costs around 150MB and no large variance.&lt;br/&gt;
&lt;br/&gt;
This variance must come from the stack memory since if I pretouch the thread stacks the numbers are very high and stable. I recently added -XX:+AlwaysPreTouchStacks; see &lt;a href=&quot;https://bugs.openjdk.org/browse/JDK-8294266&quot; title=&quot;Add a way to pre-touch java thread stacks&quot; class=&quot;issue-link&quot; data-issue-key=&quot;JDK-8294266&quot;&gt;&lt;strike&gt;JDK-8294266&lt;/strike&gt;&lt;/a&gt;. So the logic that governs how many of the splinter-small-pages remain in memory must have some outside variance factor I don&amp;#39;t understand.&lt;br/&gt;
&lt;br/&gt;
You write: &amp;quot;As for the approach of pre-allocating stacks and passing the address of stacks using pthread_attr_setstack(), the problem is that we don&amp;#39;t know when and how to de-allocate those stacks. We haven&amp;#39;t found a way to determine when the kernel is done with a thread.&amp;quot;&lt;br/&gt;
&lt;br/&gt;
This one is surprisingly difficult, that is true.&lt;br/&gt;
&lt;br/&gt;
We have `pthread_cleanup_push` [1], but that does not help since it is just a sort of finalizer for threads but requires running in the same thread.&lt;br/&gt;
&lt;br/&gt;
We have the desctructor argument of `pthread_key_create` [2], which looks *very* promising at first glance, but this destructor also runs on the thread about to exit, so this cannot be used to release the thread stack.&lt;br/&gt;
&lt;br/&gt;
The only way I see to monitor thread death reliably would be to hook into pthread_join - but that would conflict with the JDK&amp;#39;s usage of pthread_join, of course. So we would have to cooperate with the JDK and hook into its usage of pthread_join. That poses other problems, e.g., JDK may call pthread_join delayed or not at all, causing us to leak the stack.&lt;br/&gt;
&lt;br/&gt;
And all of this does not even touch the problem of performance- and stability impacts of starting to do our stack management.&lt;br/&gt;
&lt;br/&gt;
Maybe we have a better idea, but to me your idea to release stack memory with madvise seems to be the most realistic and least invasive. If only we&amp;#39;d know how much stack madvise itself needs. Maybe a large enough guess would be enough. It would be good to give some headroom for stack growth anyway since we expect the thread stack to grow.&lt;br/&gt;
</comment>
                            <comment id="14571498" author="poonam" created="Mon, 3 Apr 2023 14:25:50 -0700"  >There might be performance benefits of using huge pages for stacks, but for cases when the kernel reverts to using small pages instead of a huge page, there is no performance benefit but unnecessary bloat in RSS.&lt;br/&gt;
&lt;br/&gt;
As for the approach of pre-allocating stacks and passing the address of stacks using pthread_attr_setstack(), the problem is that we don&amp;#39;t know when and how to de-allocate those stacks. We haven&amp;#39;t found a way to determine when the kernel is done with a thread.</comment>
                            <comment id="14571495" author="poonam" created="Mon, 3 Apr 2023 14:16:27 -0700"  >[~stuefe] This is what we observe: In a pthread_create call, when memory is reserved for thread stacks, it can get backed by a THP if the thread stack size is 2MB or greater than that, and can be aligned on a 2M boundary. However, when the guard page(s) of the stack are mprotected, the 2MB range can no longer be served using a transparent huge page, and the kernel reverts back to using regular sized pages instead. At this point, the kernel does not know which pages in that 2MB range are touched and need to be resident in memory. So, it ends up keeping all the small pages in memory, thus, resulting in increased RSS for thread stacks.&lt;br/&gt;
&lt;br/&gt;
The following shows the mappings of a thread that is created with stack size 2MB. The first mapping (ffff33800000-ffff33804000) is for the guard pages. The second mapping (ffff33804000-ffff33a00000) is for the rest of the stack. Since 2032K can not be backed by a huge page, the kernel had reverted to using small pages but keeps all those pages in memory even though they are not in use. &lt;br/&gt;
&lt;br/&gt;
ffff33800000-ffff33804000 ---p 00000000 00:00 0 &lt;br/&gt;
Size:                 16 kB&lt;br/&gt;
KernelPageSize:        4 kB&lt;br/&gt;
MMUPageSize:           4 kB&lt;br/&gt;
Rss:                   0 kB&lt;br/&gt;
Pss:                   0 kB&lt;br/&gt;
Shared_Clean:          0 kB&lt;br/&gt;
Shared_Dirty:          0 kB&lt;br/&gt;
Private_Clean:         0 kB&lt;br/&gt;
Private_Dirty:         0 kB&lt;br/&gt;
Referenced:            0 kB&lt;br/&gt;
Anonymous:             0 kB&lt;br/&gt;
LazyFree:              0 kB&lt;br/&gt;
AnonHugePages:         0 kB&lt;br/&gt;
ShmemPmdMapped:        0 kB&lt;br/&gt;
FilePmdMapped:        0 kB&lt;br/&gt;
Shared_Hugetlb:        0 kB&lt;br/&gt;
Private_Hugetlb:       0 kB&lt;br/&gt;
Swap:                  0 kB&lt;br/&gt;
SwapPss:               0 kB&lt;br/&gt;
Locked:                0 kB&lt;br/&gt;
THPeligible:            0&lt;br/&gt;
VmFlags: mr mw me ac &lt;br/&gt;
ffff33804000-ffff33a00000 rw-p 00000000 00:00 0 &lt;br/&gt;
Size:               2032 kB&lt;br/&gt;
KernelPageSize:        4 kB&lt;br/&gt;
MMUPageSize:           4 kB&lt;br/&gt;
Rss:                2032 kB&lt;br/&gt;
Pss:                2032 kB&lt;br/&gt;
Shared_Clean:          0 kB&lt;br/&gt;
Shared_Dirty:          0 kB&lt;br/&gt;
Private_Clean:         0 kB&lt;br/&gt;
Private_Dirty:      2032 kB&lt;br/&gt;
Referenced:         2032 kB&lt;br/&gt;
Anonymous:          2032 kB&lt;br/&gt;
LazyFree:              0 kB&lt;br/&gt;
AnonHugePages:         0 kB&lt;br/&gt;
ShmemPmdMapped:        0 kB&lt;br/&gt;
FilePmdMapped:        0 kB&lt;br/&gt;
Shared_Hugetlb:        0 kB&lt;br/&gt;
Private_Hugetlb:       0 kB&lt;br/&gt;
Swap:                  0 kB&lt;br/&gt;
SwapPss:               0 kB&lt;br/&gt;
Locked:                0 kB&lt;br/&gt;
THPeligible:            0&lt;br/&gt;
VmFlags: rd wr mr mw me ac &lt;br/&gt;
&lt;br/&gt;
&lt;br/&gt;
However, when I create the thread with 2064K stack size (2048+16) leaving 16k aside for the guard pages, the kernel is able to use a huge page (indicated by AnonHugePages) for the rest of the stack:&lt;br/&gt;
&lt;br/&gt;
ffff68dfc000-ffff68e00000 ---p 00000000 00:00 0 &lt;br/&gt;
Size:                 16 kB&lt;br/&gt;
KernelPageSize:        4 kB&lt;br/&gt;
MMUPageSize:           4 kB&lt;br/&gt;
Rss:                   0 kB&lt;br/&gt;
Pss:                   0 kB&lt;br/&gt;
Shared_Clean:          0 kB&lt;br/&gt;
Shared_Dirty:          0 kB&lt;br/&gt;
Private_Clean:         0 kB&lt;br/&gt;
Private_Dirty:         0 kB&lt;br/&gt;
Referenced:            0 kB&lt;br/&gt;
Anonymous:             0 kB&lt;br/&gt;
LazyFree:              0 kB&lt;br/&gt;
AnonHugePages:         0 kB&lt;br/&gt;
ShmemPmdMapped:        0 kB&lt;br/&gt;
FilePmdMapped:        0 kB&lt;br/&gt;
Shared_Hugetlb:        0 kB&lt;br/&gt;
Private_Hugetlb:       0 kB&lt;br/&gt;
Swap:                  0 kB&lt;br/&gt;
SwapPss:               0 kB&lt;br/&gt;
Locked:                0 kB&lt;br/&gt;
THPeligible:            0&lt;br/&gt;
VmFlags: mr mw me ac &lt;br/&gt;
ffff68e00000-ffff69000000 rw-p 00000000 00:00 0 &lt;br/&gt;
Size:               2048 kB&lt;br/&gt;
KernelPageSize:        4 kB&lt;br/&gt;
MMUPageSize:           4 kB&lt;br/&gt;
Rss:                2048 kB&lt;br/&gt;
Pss:                2048 kB&lt;br/&gt;
Shared_Clean:          0 kB&lt;br/&gt;
Shared_Dirty:          0 kB&lt;br/&gt;
Private_Clean:         0 kB&lt;br/&gt;
Private_Dirty:      2048 kB&lt;br/&gt;
Referenced:         2048 kB&lt;br/&gt;
Anonymous:          2048 kB&lt;br/&gt;
LazyFree:              0 kB&lt;br/&gt;
AnonHugePages:      2048 kB&lt;br/&gt;
ShmemPmdMapped:        0 kB&lt;br/&gt;
FilePmdMapped:        0 kB&lt;br/&gt;
Shared_Hugetlb:        0 kB&lt;br/&gt;
Private_Hugetlb:       0 kB&lt;br/&gt;
Swap:                  0 kB&lt;br/&gt;
SwapPss:               0 kB&lt;br/&gt;
Locked:                0 kB&lt;br/&gt;
THPeligible:            1&lt;br/&gt;
VmFlags: rd wr mr mw me ac &lt;br/&gt;
</comment>
                            <comment id="14571100" author="stuefe" created="Fri, 31 Mar 2023 23:27:01 -0700"  >I would love to understand this better. If the glibc or the kernel uses TPH to back stack memory, how is this different from other memory areas? I always thought that the kernel tries to underlay existing committed memory ranges with TPH as best as possible *without* enlarging them. I may be wrong. But if I am, is this effect limited to thread stacks? Or do we also suffer from this hidden RSS enlargement also for, say, smaller-grained mmaps or malloc arenas?&lt;br/&gt;
&lt;br/&gt;
Using TPH for thread stacks may be more beneficial for stacks since thread stacks tend to be accessed often, so the performance benefit may be more visible. In other words, the excess memory spent on TPHs backing thread stacks may be justified by performance gains.&lt;br/&gt;
&lt;br/&gt;
That said, you could roll own your stack management instead of second-guessing glibc, with pthread_attr_setstackaddr. Is this what your first comment meant? You could cache the stacks to counter the perceived perf difference to glibc stack caching. Though I wonder how useful caching is - you speed up thread creation at the expense of memory kept in RSS for the sole purpose of speeding up future stack allocation. That memory may be more useful somewhere else.</comment>
                            <comment id="14569429" author="poonam" created="Fri, 24 Mar 2023 11:13:58 -0700"  >With the above approach, the madvise() call will use the current thread stack too, and for different platforms and glibc versions we can not be sure how much room should be left on the stack for the madvise() call. If we leave too less of head room for the madvise() call, we might end up removing the in-use pages from memory, and thus crashing the JVM.&lt;br/&gt;
&lt;br/&gt;
Another approach that we can follow is to make the madvise() call on an alternate temporary stack ensuring that we don&amp;#39;t advise MADV_DONTNEED for stack pages that may be in use.</comment>
                            <comment id="14564743" author="poonam" created="Mon, 6 Mar 2023 11:18:30 -0800"  >Another approach that we can follow is to instruct the kernel to remove the stack pages between the current stack pointer and the guard page:&lt;br/&gt;
&lt;br/&gt;
In the thread function, &lt;br/&gt;
1. get the stack bottom and size&lt;br/&gt;
2. get the current stack pointer&lt;br/&gt;
3. align the current pointer to the next (lower) page&lt;br/&gt;
4. get the size between the bottom and the aligned current pointer&lt;br/&gt;
5. call madvise with MADV_DONTNEED on this size&lt;br/&gt;
&lt;br/&gt;
diff --git a/src/hotspot/os/linux/os_linux.cpp b/src/hotspot/os/linux/os_linux.cpp&lt;br/&gt;
index d89388c3144..11d172abe60 100644&lt;br/&gt;
--- a/src/hotspot/os/linux/os_linux.cpp&lt;br/&gt;
+++ b/src/hotspot/os/linux/os_linux.cpp&lt;br/&gt;
@@ -697,6 +697,28 @@ static void *thread_native_entry(Thread *thread) {&lt;br/&gt;
&amp;nbsp;&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;thread-&amp;gt;record_stack_base_and_size();&lt;br/&gt;
&amp;nbsp;&lt;br/&gt;
+  pthread_attr_t tattr;&lt;br/&gt;
+  address stack_base;&lt;br/&gt;
+  address aligned_current_pointer;&lt;br/&gt;
+  size_t stack_size;&lt;br/&gt;
+&lt;br/&gt;
+  pthread_getattr_np(pthread_self(), &amp;amp;tattr);&lt;br/&gt;
+  int ret = pthread_attr_getstack(&amp;amp;tattr, (void **)&amp;amp;stack_base, &amp;amp;stack_size);&lt;br/&gt;
+  if (ret != 0) {&lt;br/&gt;
+    perror(&amp;quot;pthread_attr_getstack&amp;quot;);&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  aligned_current_pointer = (address)align_down((uintptr_t)os::current_stack_pointer(), os::linux::page_size());&lt;br/&gt;
+&lt;br/&gt;
+  size_t size = aligned_current_pointer - stack_base;&lt;br/&gt;
+ &lt;br/&gt;
+  if (madvise(stack_base, size, MADV_DONTNEED) == -1) {&lt;br/&gt;
+    perror(&amp;quot;madvise&amp;quot;);&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
&amp;nbsp;#ifndef __GLIBC__&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;// Try to randomize the cache line index of hot stack frames.&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;// This helps when threads of the same stack traces evict each </comment>
                            <comment id="14564742" author="poonam" created="Mon, 6 Mar 2023 11:16:14 -0800"  >While creating a new thread, glibc can use a cached stack instead of allocating a new one. So, if stack memory is pre-allocated (in order to madvise with MADV_NOHUGEPAGE on it) before calling pthread_create(), it can affect the performance negatively.</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10200">
                    <name>Backport</name>
                                            <outwardlinks description="backported by">
                                        <issuelink>
            <issuekey id="5102461">JDK-8309414</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="5103824">JDK-8310590</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="5103982">JDK-8310740</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10003">
                    <name>Relates</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="5103930">JDK-8310687</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="5105621">JDK-8312182</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_11700" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_10600" key="com.oracle.jira.javabugsystem-jira-plugin:jbs-fixedBackportedCustomfield">
                        <customfieldname>Fixed</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                            <customfield id="customfield_10005" key="com.atlassian.jira.plugin.system.customfieldtypes:multiselect">
                        <customfieldname>OS</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="17023"><![CDATA[linux]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                            <customfield id="customfield_11100" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i30j57:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_11004" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_10006" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Resolved In Build</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="17357"><![CDATA[b26]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_10008" key="com.oracle.jira.jira-subcomponent-plugin:oracle-subComponentField">
                        <customfieldname>Subcomponent</customfieldname>
                        <customfieldvalues>
                             <customfieldvalue key="192"><![CDATA[runtime]]></customfieldvalue> 
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_10601" key="com.oracle.jira.javabugsystem-jira-plugin:jbs-targetBackportedCustomfield">
                        <customfieldname>Targeted</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>