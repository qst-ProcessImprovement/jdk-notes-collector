<!-- 
RSS generated by JIRA (9.12.27#9120027-sha1:edc4490121e366e9e7bd2213d532dbe7e028fc5d) at Tue Sep 30 12:23:07 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>Java Bug System</title>
    <link>https://bugs.openjdk.org</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-us</language>    <build-info>
        <version>9.12.27</version>
        <build-number>9120027</build-number>
        <build-date>02-09-2025</build-date>
    </build-info>


<item>
            <title>[JDK-8269428] java/util/concurrent/ConcurrentHashMap/ToArray.java timed out</title>
                <link>https://bugs.openjdk.org/browse/JDK-8269428</link>
                <project id="10100" key="JDK">JDK</project>
                    <description>The test was started in profile &amp;#39;linux-aarch64&amp;#39; and with the following arguments: &amp;#39;-Xcomp -XX:+CreateCoredumpOnCrash -XX:+TieredCompilation&amp;#39;&lt;br/&gt;
&lt;br/&gt;
The log contains nothing bug the information about the timeout:&lt;br/&gt;
&lt;br/&gt;
result: Error. Agent error: java.lang.Exception: Agent 15 timed out with a timeout of 1920 seconds; check console log for any additional details&lt;br/&gt;
&lt;br/&gt;
A repetition of the &lt;a href=&quot;https://bugs.openjdk.org/browse/JDK-8220478&quot; title=&quot;java/util/concurrent/ConcurrentHashMap/ToArray.java timed out intermittently&quot; class=&quot;issue-link&quot; data-issue-key=&quot;JDK-8220478&quot;&gt;&lt;strike&gt;JDK-8220478&lt;/strike&gt;&lt;/a&gt; ?</description>
                <environment></environment>
        <key id="5052761">JDK-8269428</key>
            <summary>java/util/concurrent/ConcurrentHashMap/ToArray.java timed out</summary>
                <type id="1" iconUrl="https://bugs.openjdk.org/secure/viewavatar?size=xsmall&amp;avatarId=14703&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://bugs.openjdk.org/images/jbsImages/p4.png">P4</priority>
                        <status id="5" iconUrl="https://bugs.openjdk.org/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="success"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="dl">Doug Lea</assignee>
                                    <reporter username="enikitin">Evgeny Nikitin</reporter>
                        <labels>
                            <label>atr</label>
                            <label>jdk11u-fix-request</label>
                            <label>jdk11u-fix-yes</label>
                            <label>jdk17u-fix-request</label>
                            <label>jdk17u-fix-yes</label>
                            <label>jdk21u-fix-request</label>
                            <label>jdk21u-fix-yes</label>
                            <label>noreg-self</label>
                            <label>tier1</label>
                            <label>tier5</label>
                            <label>tier6</label>
                            <label>tier8</label>
                    </labels>
                <created>Sat, 26 Jun 2021 14:05:29 -0700</created>
                <updated>Mon, 24 Jun 2024 22:49:24 -0700</updated>
                            <resolved>Mon, 11 Mar 2024 05:07:05 -0700</resolved>
                                    <version>17</version>
                    <version>18</version>
                    <version>19</version>
                    <version>22</version>
                    <version>23</version>
                                    <fixVersion>23</fixVersion>
                                    <component>core-libs</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>11</watches>
                                                                                                                <comments>
                            <comment id="14684347" author="JIRAUSER20917" created="Mon, 24 Jun 2024 19:20:45 -0700"  >Fix request [11u]&lt;br/&gt;
&lt;br/&gt;
I backport this for parity with 11.0.25-oracle.&lt;br/&gt;
No risk, only tests change.&lt;br/&gt;
Test pass. SAP nightly testing passed.</comment>
                            <comment id="14683570" author="roboduke" created="Fri, 21 Jun 2024 01:15:03 -0700"  >A pull request was submitted for review.&lt;br/&gt;
URL: &lt;a href=&quot;https://git.openjdk.org/jdk11u-dev/pull/2802&quot;&gt;https://git.openjdk.org/jdk11u-dev/pull/2802&lt;/a&gt;&lt;br/&gt;
Date: 2024-06-21 08:09:59 +0000</comment>
                            <comment id="14681566" author="JIRAUSER20917" created="Thu, 13 Jun 2024 19:17:37 -0700"  >Fix request [17u]&lt;br/&gt;
&lt;br/&gt;
I backport this for parity with 17.0.13-oracle.&lt;br/&gt;
No risk, only tests change.&lt;br/&gt;
Clean backport.&lt;br/&gt;
Test pass. SAP nightly testing passed.&lt;br/&gt;
</comment>
                            <comment id="14681565" author="JIRAUSER20917" created="Thu, 13 Jun 2024 19:17:31 -0700"  >Fix request [21u]&lt;br/&gt;
&lt;br/&gt;
I backport this for parity with 21.0.5-oracle.&lt;br/&gt;
No risk, only a test change.&lt;br/&gt;
Clean backport.&lt;br/&gt;
Test pass.  SAP nightly testing passed.</comment>
                            <comment id="14681276" author="roboduke" created="Thu, 13 Jun 2024 00:31:54 -0700"  >A pull request was submitted for review.&lt;br/&gt;
URL: &lt;a href=&quot;https://git.openjdk.org/jdk17u-dev/pull/2577&quot;&gt;https://git.openjdk.org/jdk17u-dev/pull/2577&lt;/a&gt;&lt;br/&gt;
Date: 2024-06-13 07:27:02 +0000</comment>
                            <comment id="14681096" author="roboduke" created="Wed, 12 Jun 2024 20:26:04 -0700"  >A pull request was submitted for review.&lt;br/&gt;
URL: &lt;a href=&quot;https://git.openjdk.org/jdk17u-dev/pull/2575&quot;&gt;https://git.openjdk.org/jdk17u-dev/pull/2575&lt;/a&gt;&lt;br/&gt;
Date: 2024-06-13 03:21:22 +0000</comment>
                            <comment id="14681094" author="roboduke" created="Wed, 12 Jun 2024 20:25:20 -0700"  >A pull request was submitted for review.&lt;br/&gt;
URL: &lt;a href=&quot;https://git.openjdk.org/jdk21u-dev/pull/712&quot;&gt;https://git.openjdk.org/jdk21u-dev/pull/712&lt;/a&gt;&lt;br/&gt;
Date: 2024-06-13 03:20:53 +0000</comment>
                            <comment id="14656223" author="dukebot" created="Mon, 11 Mar 2024 05:07:02 -0700"  >Changeset: 570ad672&lt;br/&gt;
Author:    Viktor Klang &amp;lt;&lt;a href=&apos;mailto:vklang@openjdk.org&apos;&gt;vklang@openjdk.org&lt;/a&gt;&amp;gt;&lt;br/&gt;
Date:      2024-03-11 12:05:35 +0000&lt;br/&gt;
URL:       &lt;a href=&quot;https://git.openjdk.org/jdk/commit/570ad67204a55dd4b45e04e5a91671fed2cc18d0&quot;&gt;https://git.openjdk.org/jdk/commit/570ad67204a55dd4b45e04e5a91671fed2cc18d0&lt;/a&gt;&lt;br/&gt;
</comment>
                            <comment id="14652720" author="roboduke" created="Tue, 27 Feb 2024 02:36:39 -0800"  >A pull request was submitted for review.&lt;br/&gt;
URL: &lt;a href=&quot;https://git.openjdk.org/jdk/pull/18023&quot;&gt;https://git.openjdk.org/jdk/pull/18023&lt;/a&gt;&lt;br/&gt;
Date: 2024-02-27 10:30:44 +0000</comment>
                            <comment id="14652717" author="JIRAUSER20508" created="Tue, 27 Feb 2024 02:32:41 -0800"  >[~dl] [~jpai] The problem does not seem to be in the test itself, as switching to a dedicated ExecutorService (albeit a different implementation from the default FJP common pool) seems to make this test reliably pass.&lt;br/&gt;
&lt;br/&gt;
&lt;a href=&quot;https://github.com/openjdk/jdk/pull/18023&quot;&gt;https://github.com/openjdk/jdk/pull/18023&lt;/a&gt;&lt;br/&gt;
&lt;br/&gt;
So there&amp;#39;s most likely an underlying issue in ForkJoinPool which is triggered by this test, so that might be best to address separately.</comment>
                            <comment id="14646260" author="dcubed" created="Fri, 2 Feb 2024 10:03:27 -0800"  >[~jpai] - It is always a pleasure to read the notes from one of your elusive bug hunts!&lt;br/&gt;
Awesome job!!</comment>
                            <comment id="14646177" author="jpai" created="Fri, 2 Feb 2024 02:26:36 -0800"  >Some good news. I was finally able to reproduce this locally on my macos M1. I used the ToArray.java jtreg test to very trivially update it to include some debug messages and then created a bash script which launches this code as a standalone application and keeps re-launching it until it fails with a non-zero exit code (it will never fail). The bash script keeps writing the number of times the application was launched and if the issue is reproduced then the count won&amp;#39;t get incremented even after several minutes. I&amp;#39;ve attached the ToArray.java (no significant changes except debug logs as compared to the jtreg test) and the bash script (named &amp;quot;run.sh&amp;quot;) which runs this reproducer. If you want to reproduce it, copy the ToArray.java and run.sh to some directory locally. Set JAVA_HOME to point to latest mainline or whichever other version this issue has been reported against and then do &amp;quot;nohup bash run.sh &amp;amp;&amp;quot;. Keep watching a file named &amp;quot;status.txt&amp;quot; (which gets created by the run.sh script after the first launch completes) in the same directory - it will contain the count of number of launches of the application. If that value doesn&amp;#39;t change for a minute or so, then it&amp;#39;s a sign that the issue is reproduced. The application will be still running at this point and the logs will be available in a file called out.log. If you want you can attach visualvm or other tool against the running application to capture any state. Having said that, you may not have to run that application to reproduce it, given the following details which includes a attached heap dump file.&lt;br/&gt;
&lt;br/&gt;
On my macos M1 system Runtime.getRuntime().availableProcessors() returns 8. After running this for around an hour, and after 1535 launches from the bash script, the issue was finally reproduced. When it reproduced, the test was execution an iteration with nWorkers = 3. i.e. it was expecting 3 tasks (submitted through CompletableFuture.runAsync()) to populate a ConcurrentHashMap. Based the debug logs, which print a message when the task&amp;#39;s run() method is entered and exited, I can see that none of these 3 tasks that were submitted, ever got invoked by the common ForkJoinPool.&lt;br/&gt;
&lt;br/&gt;
I captured a heap dump of that &amp;quot;hung&amp;quot; application when this happened and it shows a very interesting thing. The ForkJoinPool has a field called &amp;quot;queues&amp;quot; which is an array of WorkQueue. Each WorkQueue will have an internal array which holds the tasks that are submitted and are available for executing. I can see that the &amp;quot;queues&amp;quot; array has 8 WorkQueue(s) of which 7 WorkQueues have a &amp;quot;owner&amp;quot; which represents a ForkJoinWorkerThread. The remaining 1 WorkQueue doesn&amp;#39;t have a &amp;quot;owner&amp;quot; (owner is null). Those 7 WorkQueues which have a &amp;quot;owner&amp;quot; are all parked waiting for a signal for any tasks to execute. However, all the 3 tasks that were submitted, have been enqueued in the sole WorkQueue which doesn&amp;#39;t have a &amp;quot;owner&amp;quot; and it appears like no one is picking work out of that queue.&lt;br/&gt;
&lt;br/&gt;
The ForkJoinPool implementation has some involved logic which determines which &amp;quot;submissionQueue&amp;quot; to enqueue the submitted tasks in and I don&amp;#39;t have knowledge of that implementation. It looks like those tasks have ended up in a wrong WorkQueue, perhaps?&lt;br/&gt;
&lt;br/&gt;
I&amp;#39;ve attached the heap dump that I captured during this run. The file is named &amp;quot;heapdump-1706857668877.hprof&amp;quot;. This test was run on JDK 23 and that heap dump was captured from that version.&lt;br/&gt;
&lt;br/&gt;
All these files have been attached within a reproducer.tar.gz in this issue.&lt;br/&gt;
&lt;br/&gt;
&lt;br/&gt;
Doug [~dl] or Martin [~martin], does this detail and the captured heap dump give some ideas on what might be going wrong on these aarch64 systems. I am not familiar with this code to take a guess on what&amp;#39;s causing this.&lt;br/&gt;
&lt;br/&gt;
</comment>
                            <comment id="14646137" author="jpai" created="Thu, 1 Feb 2024 22:39:30 -0800"  >I had a look at this today once again and I think there is some genuine issue here. So far the systems on which this test timeout has been reported have all been aarch64 (linux and macosx). Several failures have been reported with -Xcomp and many others use -XX:+UseZGC. But there have been some failure instances in this issue comments which show that this test fails with similar timeouts without any of those JVM parameters. So this isn&amp;#39;t specific to -Xcomp or any specific GC.&lt;br/&gt;
&lt;br/&gt;
At a high level, the test does the following:&lt;br/&gt;
&lt;br/&gt;
- Runs in agent vm mode with &amp;quot;main()&amp;quot; method as the entry point&lt;br/&gt;
- The main() method runs the same test operations 10 times&lt;br/&gt;
- For each of those iterations:&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;- The test determines how many &amp;quot;workers&amp;quot; to use to run a specific task. The number of worker threads will be between 2 and Runtime.getRuntime().availableProcessors() (both inclusive). If the Runtime.getRuntime().availableProcessors() is greater than 32, then the test limits the maximum number of workers to 32.&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;- Each of the N workers will be populating the same instance of a ConcurrentHashMap &amp;quot;m&amp;quot; using m.put(i, j). Each of these N worker tasks will populate 1024 unique entries. So at the end of these N tasks, there should be a N * 1024 entries in the ConcurrentHashMap &amp;quot;m&amp;quot;. This N * 1024 is considered the &amp;quot;maxSize&amp;quot;. Each of these N tasks are submitted using CompletableFuture.runAsync(...) which internally uses the common ForkJoinPool.&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;- Apart from those N tasks, the test also submits one single task, through CompletableFuture.runAsync(...) which is responsible for checking the progress of the ConcurrentHashMap. Checking of progress involves checking the size of the arrays returned by m.keySet() and m.values(). This task expects the size of such arrays to regularly increase until the ConcurrentHashMap has been populated with the expected &amp;quot;maxSize&amp;quot; entries. This task doesn&amp;#39;t complete till the ConcurrentHashMap size reaches that maxSize. This task too gets launched through the common ForkJoinPool.&lt;br/&gt;
&lt;br/&gt;
- Once these N worker tasks and the 1 &amp;quot;foreman&amp;quot; task (the one which checks the progress) are submitted, the test first does a &amp;quot;workers.forEach(CompletableFuture&amp;lt;?&amp;gt;::join);&amp;quot; i.e. waits on all submitted N worker tasks (the ones which populate the ConcurrentHashMap) to complete and then does &amp;quot;foreman.join();&amp;quot; i.e. waits for the progress checking task to complete.&lt;br/&gt;
&lt;br/&gt;
&lt;br/&gt;
All these reported test failures are with a timeout, i.e. the test never completes (sometimes as long as 20 minutes or more). I have looked at the jstack output of several of the linked failures in the JBS issue. There&amp;#39;s one common pattern in all of them. I&amp;#39;ll paste the relevant thread stacks here from one such run. For reference, I&amp;#39;ve attached a copy of that thread dump to this issue, file named &amp;quot;jstack-2962696.txt&amp;quot;. This specific instance of timeout and the corresponding jstack output is from a recent run against the JDK mainline (JDK 23).&lt;br/&gt;
&lt;br/&gt;
The threads of interest are the ones named &amp;quot;ForkJoinPool.commonPool-worker...&amp;quot; and the &amp;quot;AgentVMThread&amp;quot; thread. The &amp;quot;AgentVMThread&amp;quot; thread is the one in which the test&amp;#39;s &amp;quot;main()&amp;quot; method is running. The &amp;quot;ForkJoinPool.commonPool-worker...&amp;quot; threads are the ones used for running the &amp;quot;foreman&amp;quot; (progress checker) thread and N worker threads (responsible for populating the ConcurrentHashMap). You will notice that there are 7 &amp;quot;ForkJoinPool.commonPool-worker...&amp;quot; threads, from &amp;quot;ForkJoinPool.commonPool-worker-1&amp;quot; through &amp;quot;ForkJoinPool.commonPool-worker-7&amp;quot;. Except one of these &amp;quot;ForkJoinPool.commonPool-worker...&amp;quot; ForkJoinPool threads, the rest of them all are awaiting work:&lt;br/&gt;
&lt;br/&gt;
...&lt;br/&gt;
&amp;nbsp;&amp;nbsp;java.lang.Thread.State: WAITING (parking)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at jdk.internal.misc.Unsafe.park(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/Native Method)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;- parking to wait for  &amp;lt;0x00000000d032cc50&amp;gt; (a java.util.concurrent.ForkJoinPool)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.util.concurrent.ForkJoinPool.awaitWork(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/ForkJoinPool.java:2145)&lt;br/&gt;
...&lt;br/&gt;
&lt;br/&gt;
The one &amp;quot;ForkJoinPool.commonPool-worker...&amp;quot; thread which is actually running a task is the &amp;quot;ForkJoinPool.commonPool-worker-3&amp;quot; which is doing:&lt;br/&gt;
&lt;br/&gt;
&amp;quot;ForkJoinPool.commonPool-worker-3&amp;quot; #116 [2968288] daemon prio=5 os_prio=0 cpu=447000.77ms elapsed=544.96s allocated=2023G defined_classes=2 tid=0x0000fffed8119b40 nid=2968288 runnable  [0x0000ffff19e1c000]&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;java.lang.Thread.State: RUNNABLE&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at ToArray$1.run(ToArray.java:82)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.util.concurrent.CompletableFuture$AsyncRun.run(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/CompletableFuture.java:1804)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.util.concurrent.CompletableFuture$AsyncRun.exec(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/CompletableFuture.java:1796)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.util.concurrent.ForkJoinTask.doExec(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/ForkJoinTask.java:507)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/ForkJoinPool.java:1491)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.util.concurrent.ForkJoinPool.scan(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/ForkJoinPool.java:2073)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.util.concurrent.ForkJoinPool.runWorker(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/ForkJoinPool.java:2035)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.util.concurrent.ForkJoinWorkerThread.run(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/ForkJoinWorkerThread.java:187)&lt;br/&gt;
&lt;br/&gt;
&lt;br/&gt;
Line 82 of ToArray$1.run corresponds to the &amp;quot;foreman&amp;quot; task (the one which does the progress checking). So this &amp;quot;ForkJoinPool.commonPool-worker-3&amp;quot; thread is the progress checking thread and is constantly checking the size of the ConcurrentHashMap. What this implies is that even after a very long duration, the progress checker hasn&amp;#39;t yet found the ConcurrentHashMap to contain the expected N * 1024 entries.&lt;br/&gt;
&lt;br/&gt;
The other crucial part of this thread dump is the &amp;quot;AgentVMThread&amp;quot; (the one in which the test&amp;#39;s main() method is running). It shows this:&lt;br/&gt;
&lt;br/&gt;
&lt;br/&gt;
&amp;quot;AgentVMThread&amp;quot; #706 [2971151] prio=5 os_prio=0 cpu=3.89ms elapsed=492.36s allocated=71864B defined_classes=6 tid=0x0000ffff7c2dd6c0 nid=2971151 waiting on condition  [0x0000fffe5b5b7000]&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;java.lang.Thread.State: WAITING (parking)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at jdk.internal.misc.Unsafe.park(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/Native Method)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;- parking to wait for  &amp;lt;0x00000000d09177b0&amp;gt; (a java.util.concurrent.CompletableFuture$Signaller)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.util.concurrent.locks.LockSupport.park(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/LockSupport.java:221)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.util.concurrent.CompletableFuture$Signaller.block(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/CompletableFuture.java:1864)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.util.concurrent.ForkJoinPool.unmanagedBlock(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/ForkJoinPool.java:4013)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.util.concurrent.ForkJoinPool.managedBlock(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/ForkJoinPool.java:3961)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.util.concurrent.CompletableFuture.waitingGet(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/CompletableFuture.java:1898)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.util.concurrent.CompletableFuture.join(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/CompletableFuture.java:2117)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at ToArray$$Lambda/0x00000000251acc58.accept(Unknown Source)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.util.ArrayList.forEach(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/ArrayList.java:1597)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at ToArray.executeTest(ToArray.java:98)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at ToArray.main(ToArray.java:44)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.lang.invoke.LambdaForm$DMH/0x00000000250c0000.invokeStatic(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/LambdaForm$DMH)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.lang.invoke.LambdaForm$MH/0x0000000025280000.invoke(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/LambdaForm$MH)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.lang.invoke.Invokers$Holder.invokeExact_MT(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/Invokers$Holder)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at jdk.internal.reflect.DirectMethodHandleAccessor.invokeImpl(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/DirectMethodHandleAccessor.java:154)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/DirectMethodHandleAccessor.java:103)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.lang.reflect.Method.invoke(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/Method.java:580)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at com.sun.javatest.regtest.agent.MainActionHelper$AgentVMRunnable.run(MainActionHelper.java:333)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.lang.Thread.runWith(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/Thread.java:1583)&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.lang.Thread.run(&lt;a href=&apos;mailto:java.base@23-ea&apos;&gt;java.base@23-ea&lt;/a&gt;/Thread.java:1570)&lt;br/&gt;
&lt;br/&gt;
&lt;br/&gt;
The important bit in that thread dump is this line &amp;quot;at ToArray.executeTest(ToArray.java:98)&amp;quot; which corresponds to &amp;quot;workers.forEach(CompletableFuture&amp;lt;?&amp;gt;::join);&amp;quot; in the test code. So this thread is waiting for the worker threads to complete populating the ConcurrentHashMap, but they haven&amp;#39;t yet completed (even after this long duration). So in theory, in that thread dump, there should have been one or more ForkJoinPool threads which should have contained a stackframe showing the worker thread(s) populating the ConcurrentHashMap. But as we saw above, all the remaining 6 ForkJoinPool threads are just awaiting work and aren&amp;#39;t running any task. &lt;br/&gt;
&lt;br/&gt;
The fact that the AgentVMThread is waiting on &amp;quot;workers.forEach(CompletableFuture&amp;lt;?&amp;gt;::join);&amp;quot; tells us that the &amp;quot;foreman&amp;quot; thread (the one checking progress) is correctly waiting for the ConcurrentHashMap entries size to reach the expected N * 1024 entries. So we can rule out the toArray() calls in the foreman threads or any other code in that thread to be causing an issue here. So that then means, that there&amp;#39;s some bug within the common ForkJoinPool that is manifesting only on aarch64 platforms, that causes tasks to not get fired or not get marked as completed. I think we can rule out the latter part (i.e. task is getting fired but not marked as completed), because if that was the case then the &amp;quot;foreman&amp;quot; thread would at least have completed after noticing the ConcurrentHashMap entries were rightly populated, but that isn&amp;#39;t happening. So that then leaves us with only one theory that on aarch64 when CompletableFuture.runAsync() internally uses the common ForkJoinPool, in some cases it never executes (one or more) tasks that have been submitted.&lt;br/&gt;
&lt;br/&gt;
I also have looked at the test artifacts of some of these timing out jobs to see if any of the other tests that may have run on this specific (reused) agent JVM might have done something to the common ForkJoinPool that could contribute to this. Specifically, I have checked if any of them call setParallelism(...) on the common ForkJoinPool and I haven&amp;#39;t found any such calls. So the chances of some previously run test causing this seems unlikely.&lt;br/&gt;
&lt;br/&gt;
I have tried to reproduce this on mach5 and locally on my macosx M1 but haven&amp;#39;t been able to so far. I plan to trigger a local long running run with some debug logs in the test to see if I can reproduce this and get some data. At this point though, based on the code and the thread dump, it appears that the common ForkJoinPool isn&amp;#39;t triggering one or more tasks that have been submitted to it, on aarch64.</comment>
                            <comment id="14636670" author="dholmes" created="Mon, 1 Jan 2024 13:50:09 -0800"  >The Java thread dump brings the VM to a safepoint to take the dump.&lt;br/&gt;
&lt;br/&gt;
The native thread dump does not indicate we are at a safepoint. The VM thread is waiting for an operation. There is at least one Java thread still active&lt;br/&gt;
&lt;br/&gt;
&amp;nbsp;&amp;nbsp;thread #29, name = &amp;#39;Java: ForkJoinPool.commonPool-worker-1&amp;#39;&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;frame #0: 0x000000018c5d7494 libsystem_platform.dylib`_platform_memset_pattern16 + 84&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;frame #1: 0x0000000105cd4bf0 libjvm.dylib`MemAllocator::mem_allocate_inside_tlab_slow(MemAllocator::Allocation&amp;amp;) const + 512&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;frame #2: 0x0000000105cd5330 libjvm.dylib`MemAllocator::allocate() const + 140&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;frame #3: 0x00000001057cd3a8 libjvm.dylib`InstanceKlass::allocate_instance(JavaThread*) + 184&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;frame #4: 0x0000000105ebeb38 libjvm.dylib`OptoRuntime::new_instance_C(Klass*, JavaThread*) + 1220</comment>
                            <comment id="14636017" author="dcubed" created="Thu, 21 Dec 2023 06:57:58 -0800"  >Here&amp;#39;s a log file snippet from the jdk-23+3-133-tier6 sighting:&lt;br/&gt;
&lt;br/&gt;
java/util/concurrent/ConcurrentHashMap/ToArray.java&lt;br/&gt;
&lt;br/&gt;
#section:main&lt;br/&gt;
----------messages:(10/307)----------&lt;br/&gt;
command: main ToArray&lt;br/&gt;
reason: Assumed action based on file name: run main ToArray &lt;br/&gt;
started: Thu Dec 21 12:28:57 GMT 2023&lt;br/&gt;
Mode: agentvm&lt;br/&gt;
Agent id: 16&lt;br/&gt;
Timeout signalled after 1200 seconds&lt;br/&gt;
Timeout information:&lt;br/&gt;
--- Timeout information end.&lt;br/&gt;
finished: Thu Dec 21 12:51:40 GMT 2023&lt;br/&gt;
elapsed time (seconds): 1363.726&lt;br/&gt;
----------configuration:(12/1552)----------&lt;br/&gt;
&lt;br/&gt;
&amp;lt;snip&amp;gt;&lt;br/&gt;
&lt;br/&gt;
result: Error. Agent error: java.lang.Exception: Agent 16 timed out with a timeout of 1200 seconds; check console log for any additional details&lt;br/&gt;
&lt;br/&gt;
Again there are no indicators in the log to explain why the test timed out.</comment>
                            <comment id="14630826" author="dcubed" created="Sun, 3 Dec 2023 06:26:20 -0800"  >Here&amp;#39;s a log file snippet from the jdk-22+27-2172-tier5 sighting:&lt;br/&gt;
&lt;br/&gt;
java/util/concurrent/ConcurrentHashMap/ToArray.java&lt;br/&gt;
&lt;br/&gt;
#section:main&lt;br/&gt;
----------messages:(10/304)----------&lt;br/&gt;
command: main ToArray&lt;br/&gt;
reason: Assumed action based on file name: run main ToArray &lt;br/&gt;
started: Sun Dec 03 04:35:01 UTC 2023&lt;br/&gt;
Mode: agentvm&lt;br/&gt;
Agent id: 7&lt;br/&gt;
Timeout signalled after 480 seconds&lt;br/&gt;
Timeout information:&lt;br/&gt;
--- Timeout information end.&lt;br/&gt;
finished: Sun Dec 03 04:44:12 UTC 2023&lt;br/&gt;
elapsed time (seconds): 550.666&lt;br/&gt;
----------configuration:(12/1474)----------&lt;br/&gt;
&lt;br/&gt;
&amp;lt;snip&amp;gt;&lt;br/&gt;
&lt;br/&gt;
result: Error. Agent error: java.lang.Exception: Agent 7 timed out with a timeout of 480 seconds; check console log for any additional details&lt;br/&gt;
&lt;br/&gt;
No indicators of any issues in the log file...</comment>
                            <comment id="14625203" author="dholmes" created="Thu, 9 Nov 2023 03:56:24 -0800"  >We&amp;#39;ve started seeing this fail again in our tier5 testing which uses GenZGC - 2 times in 4 days. Still on linux-aarch64.&lt;br/&gt;
&lt;br/&gt;
----------messages:(10/305)----------&lt;br/&gt;
command: main ToArray&lt;br/&gt;
reason: Assumed action based on file name: run main ToArray &lt;br/&gt;
started: Thu Nov 09 10:49:43 UTC 2023&lt;br/&gt;
Mode: agentvm&lt;br/&gt;
Agent id: 30&lt;br/&gt;
Timeout signalled after 480 seconds&lt;br/&gt;
Timeout information:&lt;br/&gt;
--- Timeout information end.&lt;br/&gt;
finished: Thu Nov 09 10:59:19 UTC 2023&lt;br/&gt;
elapsed time (seconds): 576.095</comment>
                            <comment id="14464944" author="martin" created="Thu, 16 Dec 2021 18:54:41 -0800"  >This is a rare failure where jtreg is failing to communicate with one of its own agents, which should never happen.&lt;br/&gt;
I&amp;#39;ve seen such failures for many years, all very rare and non-reproducible.&lt;br/&gt;
I&amp;#39;ve always considered this to be some bug in jtreg, or at least there is missing diagnostic code in jtreg.&lt;br/&gt;
What happened to the agent??&lt;br/&gt;
It&amp;#39;s not a dup of &lt;a href=&quot;https://bugs.openjdk.org/browse/JDK-8220478&quot; title=&quot;java/util/concurrent/ConcurrentHashMap/ToArray.java timed out intermittently&quot; class=&quot;issue-link&quot; data-issue-key=&quot;JDK-8220478&quot;&gt;&lt;strike&gt;JDK-8220478&lt;/strike&gt;&lt;/a&gt; because of where the failure is.&lt;br/&gt;
From &amp;quot;Agent id: 21&amp;quot; I suspect that the cause is related to a very high degree of concurrency within jtreg itself, which may be overwhelming the aarch64 machine.  Plausibly, the machine is memory constrained and the Linux OOM killer decides to kill an agent process, with inadequate reporting.</comment>
                            <comment id="14464604" author="dcubed" created="Wed, 15 Dec 2021 07:44:50 -0800"  >The log file for the jdk-19+2-33-tier6 sighting has been attached as: &lt;br/&gt;
&lt;br/&gt;
mach5-one-jdk-19+2-33-tier6-20211215-0835-27304424-tier6-comp-open_test_jdk_jdk_util-linux-aarch64-debug-181-1639568049-515.log</comment>
                            <comment id="14463810" author="dcubed" created="Fri, 10 Dec 2021 11:05:50 -0800"  >The log file for the jdk-18+28-1957-tier6 sighting has been attached as:&lt;br/&gt;
mach5-one-jdk-18+28-1957-tier6-20211210-1524-27138005-tier6-comp-open_test_jdk_jdk_util-linux-aarch64-debug-181-1639162354-515.log</comment>
                            <comment id="14458302" author="dl" created="Sun, 14 Nov 2021 08:48:55 -0800"  >I can&amp;#39;t replicate. But in the course of updates for &lt;a href=&quot;https://bugs.openjdk.org/browse/JDK-8277090&quot; title=&quot;jsr166 refresh for jdk19&quot; class=&quot;issue-link&quot; data-issue-key=&quot;JDK-8277090&quot;&gt;&lt;strike&gt;JDK-8277090&lt;/strike&gt;&lt;/a&gt; (&lt;a href=&quot;https://bugs.openjdk.java.net/browse/JDK-8277090),&quot;&gt;https://bugs.openjdk.java.net/browse/JDK-8277090),&lt;/a&gt; I discovered and fixed an inconsistency in termination checks that could be responsible.&lt;br/&gt;
</comment>
                            <comment id="14457001" author="dcubed" created="Fri, 5 Nov 2021 19:51:46 -0700"  >Here&amp;#39;s a log file snippet from the jdk-18+23-1456-tier6 sighting:&lt;br/&gt;
&lt;br/&gt;
#section:main&lt;br/&gt;
----------messages:(8/220)----------&lt;br/&gt;
command: main ToArray&lt;br/&gt;
reason: Assumed action based on file name: run main ToArray &lt;br/&gt;
Mode: agentvm&lt;br/&gt;
Agent id: 21&lt;br/&gt;
Timeout refired 1200 times&lt;br/&gt;
Timeout information:&lt;br/&gt;
--- Timeout information end.&lt;br/&gt;
elapsed time (seconds): 1227.998&lt;br/&gt;
----------configuration:(14/1643)----------&lt;br/&gt;
&lt;br/&gt;
&amp;lt;snip&amp;gt;&lt;br/&gt;
&lt;br/&gt;
result: Error. Agent error: java.lang.Exception: Agent 21 timed out with a timeout of 1200 seconds; check console log for any additional details&lt;br/&gt;
</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10200">
                    <name>Backport</name>
                                            <outwardlinks description="backported by">
                                        <issuelink>
            <issuekey id="5131414">JDK-8333870</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="5131656">JDK-8334088</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="5131769">JDK-8334192</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="5131959">JDK-8334347</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="5131992">JDK-8334375</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="5132601">JDK-8334920</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10003">
                    <name>Relates</name>
                                                                <inwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="4985346">JDK-8220478</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="108115" name="jstack-2962696.txt" size="19281" author="jpai" created="Thu, 1 Feb 2024 22:43:50 -0800"/>
                            <attachment id="97336" name="mach5-one-jdk-18+28-1957-tier6-20211210-1524-27138005-tier6-comp-open_test_jdk_jdk_util-linux-aarch64-debug-181-1639162354-515.log" size="17553" author="dcubed" created="Fri, 10 Dec 2021 11:05:11 -0800"/>
                            <attachment id="97384" name="mach5-one-jdk-19+2-33-tier6-20211215-0835-27304424-tier6-comp-open_test_jdk_jdk_util-linux-aarch64-debug-181-1639568049-515.log" size="17452" author="dcubed" created="Wed, 15 Dec 2021 07:44:20 -0800"/>
                            <attachment id="108116" name="reproducer.tar.gz" size="2715708" author="jpai" created="Fri, 2 Feb 2024 02:27:12 -0800"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_10000" key="com.atlassian.jira.plugin.system.customfieldtypes:multiselect">
                        <customfieldname>CPU</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="19300"><![CDATA[aarch64]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_11700" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_10600" key="com.oracle.jira.javabugsystem-jira-plugin:jbs-fixedBackportedCustomfield">
                        <customfieldname>Fixed</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_11100" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2tdk3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_11004" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_10006" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Resolved In Build</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="17407"><![CDATA[b14]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_10008" key="com.oracle.jira.jira-subcomponent-plugin:oracle-subComponentField">
                        <customfieldname>Subcomponent</customfieldname>
                        <customfieldvalues>
                             <customfieldvalue key="215"><![CDATA[java.util.concurrent]]></customfieldvalue> 
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_10601" key="com.oracle.jira.javabugsystem-jira-plugin:jbs-targetBackportedCustomfield">
                        <customfieldname>Targeted</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>